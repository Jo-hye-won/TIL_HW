버전 1 : Django + D.S(데이터사이언스)
버전 2 : Django 기초 + 인증(회원)


# 웹 크롤링 실습
## 파이썬으로 웹 페이지에 있는 정보를 가져오는 방법
- 크게 세 가지 방법으로 가져올 수 있다. 
1. 누군가 업로드해 둔 데이터를 다운로드 받기 (ex. 캐글)
2. 누군가 만들어 둔 API Server를 활용하여 정보를 받아오기
3. 사람이 검색하는 것처럼 파이썬이 자동으로 검색 후 결과를 수집하는 방법
    - 이러한 기술을 크롤링(Crawling)이라고 함
    - 이번 프로젝트에서 사용할 기술

## 데이터 사이언스 프로세스
- 필요한 정보를 추출하는 5가지 단계
1. 문제 정의 : 해결하고자 하는 문제 정의
2. 데이터 수집 : 문제 해결에 필요한 데이터 수집
3. 데이터 전처리(정제) : 실질적인 분석을 수행하기 위해 데이터를 가공하는 단계 
    - 수집한 데이터의 오류 제거(결측치, 이상치), 데이터 형식 변환 등
4. 데이터 분석 : 전처리가 완료된 데이터에서 필요한 정보를 추출하는 단계
5. 결과 해석 및 공유 : 의사 결정에 활용하기 위해 결과를 해석하고 시각화 후 공유하는 단계 

## 데이터 수집
- 웹 스크래핑 : 웹 페이지에서 데이터를 추출하는 기술
- 웹 크롤링 : 웹 페이지를 자동으로 탐색하고 데이터를 수집하는 기술
- Open API활용 : 공개딘 API를 통해 데이터를 수집
- 데이터 공유 플랫폼 활용 : 다양한 사용자가 데이터를 공유하고 활용할 수 있는 온라인 플랫폼
    - 종류 : 캐글, Data.world, 데이콘, 공공데이터포털 등


## 웹 크롤링이란?
- 여러 웹 페이지를 돌아다니며 원하는 정보를 모으는 기술
- 원하는 정보를 추출하는 스크래핑과 여러 웹 페이지를 자동으로 탐색하는 크롤링의 개념을 합쳐 웹 크롤링이라고 부름
- 즉, 웹 사이트들을 돌아다니며 필요한 데이터를 추출하여 활용할 수 있도록 자동화된 프로세스

## 웹 크롤링 프로세스
1. 웹 페이지 다운로드 : 해당 웹 페이지의 HTML, CSS, JavaScript등의 코드를 가져오는 단계 
2. 페이지 파싱 : 다운로드 받은 코드를 분석하고 필요한 데이터를 추출하는 단계 
3. 링크 추출 및 다른 페이지 탐색 : 다른 링크를 추출하고, 다음 단계로 이동하여 원하는 데이터를 추출하는 단계 
4. 데이터 추출 및 저장 : 분석 및 시각화에 사용하기 이해 데이터를 처리하고 저장하는 단계 

### 준비단계 
- 실습 및 도전과제에는 구글 검색 결과 페이지를 크롤링
- 아래 필수 라이브러리 설치 후 진행
    - requests : HTTP 요청을 보내고 응답을 받을 수 있는 모듈
    - BeautifulSoup : HTML 문서에서 원하는 데이터를 추출하는 데 사용되는 파이썬 라이브러리
    - Selenium : 웹 애플리케이션을 테스트하고 자동화하기 위한 파이썬 라이브러리
        - 웹 페이지의 동적인 컨텐츠를 가져오기 위해 사용함(검색 결과 등)

```bash
pip install requests beautifulsoup4 selenium
```


[Quotes to Scrape](https://quotes.toscrape.com/)


주소 뒤에 robots.txt 쳐보면 신경써야 할 저작권 나옴(allow / disallow)

버전 1 : Django + 크롤링 (views.py에 작성) + 구글링
버전 2 : Django + 인증


git push origin hyewon

